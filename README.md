**为 12 月 13 日（周日）**

大家每周日发送邮件简单总结一下本周完成了哪些内容，最后在截止日期前将代码、文档、演示视频打包（命名“学号
_ 姓名 _hw6”）发送到nkulixuy@163.com。

本次作业的要求是针对南开校内网站构建一个 Web 搜索引擎，为用户提供南开信息的查询服
务乃至个性化推荐。本次作业可以借助各种工具和包，希望大家善于利用以减少工作量。
1 具体实现
实现这次作业主要有网页抓取、文本索引、链接分析、查询服务、个性化查询几个步骤，个性
化推荐为扩展内容。
1.1 网页抓取
对南开大学各网页内容进行抓取。
1.2 文本索引
对网页及其锚文本构建索引，可以按锚文本、网页标题、URL 等域构建索引。
1.3 链接分析
使用 PageRank 对链接进行分析，评估网页权重
1.4 查询服务
使用向量空间模型并结合链接分析对查询结果进行排序，为用户提供站内查询、文档查询、短
语查询、通配查询、查询日志、网页快照等高级搜索功能。更多的内容可以参考百度（图 1) 或谷歌
（图 2）的高级搜索功能。
1.5 个性化查询
个性化查询为不同的用户提供不同的内容排序。可以实现一个账号登录系统，通过用户完善的
学院专业等个人信息为其呈现不同的查询结果；或者是记录用户的查询历史，通过历史查询来提供
个性化的查询结果。在 google 的查询中就会通过这些手段来优化用户的查询体验
1.6 个性化推荐
本次作业的扩展内容为个性化推荐，个性化推荐系统通过用户的个人信息和查询历史获取用户
可能的兴趣点，在用户查询时给用户推荐相关领域的其他内容。比如在百度上搜索 iphone，其会在
查询结果的右侧为你推荐 ipad、iMac 等相关产品（图 4）。

sdu视点新闻全站爬虫爬取+索引构建+搜索引擎查询练习程序

爬虫功能使用Python的scrapy库实现，并用MongoDB数据库进行存储。

索引构建和搜索功能用Python的Whoosh和jieba库实现。（由于lucene是java库，所以pyLucene库的安装极其麻烦，因此选用Python原生库Whoosh实现，并使用jieba进行中文分词。）

搜索网页界面用django实现，页面模板套用BootCDN。

1 要求

以下是检索的基本要求：可以利用lucene、nutch等开源工具，利用Python、Java等编程语言，但需要分别演示并说明原理。

1.1 Web网页信息抽取

以山东大学新闻网为起点进行网页的循环爬取，保持爬虫在view.sdu.edu.cn之内（即只爬取这个站点的网页），爬取的网页数量越多越好。

1.2 索引构建

对上一步爬取到的网页进行结构化预处理，包括基于模板的信息抽取、分字段解析、分词、构建索引等。

1.3 检索排序

对上一步构建的索引库进行查询，对于给定的查询，给出检索结果，明白排序的原理及方法。




# EverythinNKU

## web crawler

1. 获取nk vpn首页urls，作为种子url
2. 根据种子url，爬取网页，将该url加入used url列表，
    将网页中的链接加入到unused url列表，防止重复爬取。
3. 将网页中重要的信息（一般包含在a标签下）保存为文件供建立索引

文件格式
filename=base url.html


**做的优化**
1. 多线程
2. 锁机制
3. 日志（还没完成）


## index

## 难点
暗网/动态资源爬取：一般的爬虫无法爬取搜索引擎中的信息