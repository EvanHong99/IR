<!--
**为 12 月 13 日（周日）**

大家每周日发送邮件简单总结一下本周完成了哪些内容，最后在截止日期前将代码、文档、演示视频打包（命名“学号
_ 姓名 _hw6”）发送到nkulixuy@163.com。

本次作业的要求是针对南开校内网站构建一个 Web 搜索引擎，为用户提供南开信息的查询服
务乃至个性化推荐。本次作业可以借助各种工具和包，希望大家善于利用以减少工作量。
1 具体实现
实现这次作业主要有网页抓取、文本索引、链接分析、查询服务、个性化查询几个步骤，个性
化推荐为扩展内容。
1.1 网页抓取
对南开大学各网页内容进行抓取。
1.2 文本索引
对网页及其锚文本构建索引，可以按锚文本、网页标题、URL 等域构建索引。
1.3 链接分析
使用 PageRank 对链接进行分析，评估网页权重
1.4 查询服务
使用向量空间模型并结合链接分析对查询结果进行排序，为用户提供站内查询、文档查询、短
语查询、通配查询、查询日志、网页快照等高级搜索功能。更多的内容可以参考百度（图 1) 或谷歌
（图 2）的高级搜索功能。
1.5 个性化查询
个性化查询为不同的用户提供不同的内容排序。可以实现一个账号登录系统，通过用户完善的
学院专业等个人信息为其呈现不同的查询结果；或者是记录用户的查询历史，通过历史查询来提供
个性化的查询结果。在 google 的查询中就会通过这些手段来优化用户的查询体验
1.6 个性化推荐
本次作业的扩展内容为个性化推荐，个性化推荐系统通过用户的个人信息和查询历史获取用户
可能的兴趣点，在用户查询时给用户推荐相关领域的其他内容。比如在百度上搜索 iphone，其会在
查询结果的右侧为你推荐 ipad、iMac 等相关产品（图 4）。

sdu视点新闻全站爬虫爬取+索引构建+搜索引擎查询练习程序

爬虫功能使用Python的scrapy库实现，并用MongoDB数据库进行存储。

索引构建和搜索功能用Python的Whoosh和jieba库实现。（由于lucene是java库，所以pyLucene库的安装极其麻烦，因此选用Python原生库Whoosh实现，并使用jieba进行中文分词。）

搜索网页界面用django实现，页面模板套用BootCDN。

1 要求

以下是检索的基本要求：可以利用lucene、nutch等开源工具，利用Python、Java等编程语言，但需要分别演示并说明原理。

1.1 Web网页信息抽取

以山东大学新闻网为起点进行网页的循环爬取，保持爬虫在view.sdu.edu.cn之内（即只爬取这个站点的网页），爬取的网页数量越多越好。

1.2 索引构建

对上一步爬取到的网页进行结构化预处理，包括基于模板的信息抽取、分字段解析、分词、构建索引等。

1.3 检索排序

对上一步构建的索引库进行查询，对于给定的查询，给出检索结果，明白排序的原理及方法。
-->




# EverythinNKU

**主要实现功能**
- 网页爬取
- 倒排索引构建
- 利用PageRank将结果排序进一步优化
- 自己编写的VSM排序查询结果

## web crawler

[查看源码](CRAWLER.py)

1. 将南开大学官网作为种子url
2. 根据种子url，自己编写程序进行网页爬取，将该url加入used url列表，
    将网页中的链接加入到unused url列表，防止重复爬取。
3. 将网页中重要的信息（一般包含在a标签下），比如标题、锚文本、相对链接等保存为数据库文件供建立索引。
将文本文件根据相应的映射存储为txt文件

**做的优化**
1. 多线程
2. 锁机制
3. 爬取日志



## index

[查看源码](BSBI.py)

以BSBI为基础，在parse block的同时进行自定义目标数据结构的维护，并用pickle、json等包的dump函数将其保存到磁盘中。

下次使用时，再利用load函数将其加载到内存。

类内主要多维护了这几个属性：
```python
self.term_tf_docid = defaultdict(lambda: [0, set()])
self.term_idf = defaultdict(float)
self.docid_terms_tfs = defaultdict(lambda: defaultdict(int))
```
分别用来存储词向的倒排索引及其词频、词向的逆文档频率、文档内词向的词频

## PageRank
[查看源码](PageSorter.py)

1. 根据爬取的链接间的关系，构建有向图
2. 利用networkx进行PageRank计算
3. 将结果保存到磁盘中

## VSM

[查看源码](PageSorter.py)

1. 继承自BSBIIndex，主要将之前计算出来的tf、idf进行最后的相乘求和并归一化，
并扩展了对外的查询接口，能够提供文档查询个性化查询等服务。
2. 此外，会对每位用户的查询进行日志记录，便于提供更好地个性化查询。现阶段的个性化查询比较简单粗暴，
日后会对其算法进行进一步的改进。


## 遇到的问题

**网页爬取内容一直会出现各种各样的问题**

这是一个不断发现问题并解决问题的过程，现在我已经爬取的数据中还是会有很多没有考虑到的问题。日后会进一步改进。

**对Python语言的掌握度不够，发生错误但是一直没有发现**

```python
uniq_urls = [i for i in set(new_urls) if i not in unused_url and used_url]
```

该语句并不能将i排除在两个列表之外，而是只能将i排除在第一个列表之外。

此bug可能是导致我数据库内容大量重复的原因之一。

修改为

```python
uniq_urls = [i for i in set(new_urls) if i not in unused_url and i not in used_url]
```

**避免死锁**

编写acquire函数，给锁进行相应的编号，使得两个锁的获得必定是有序的

**相对链接地址**

```html
<div class="text-muted description">
    <p class="text"><a href='/2020/1114/c19665a317736/page.htm' target='_blank' title='金融学院召开课程思政建设工作推动会'>（通讯员：徐静）为进一步贯彻落实《南开大学课程思政建设实施方案》和南开大学课...</a></p>
</div>
```