{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业一：布尔检索\n",
    "### 实现及改进的点\n",
    "- 利用pd.array().unique()来实现列表的去重\n",
    "- 实现了多个and的优化\n",
    "\n",
    "### 遇到的困难\n",
    "- np.array()的自动类型转换会导致列表list连接运算符+变成列表项相加\n",
    "- (based&&!day)||(based&&day)最外边的括号对符合语法，但是这里不需要去掉括号\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对文本进行分词\n",
    "使用了NLTK工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    text = re.sub(r\"[{}]+\".format(punctuation), \" \", text)  # 将标点符号转化为空格\n",
    "    text = text.lower()  # 全部字符转为小写\n",
    "    words = nltk.word_tokenize(text)  # 分词\n",
    "    words = list(set(words).difference(set(sw)))  # 去停用词\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取文本文件\n",
    "给定文本文件目录，获取目录下所有符合要求的文件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_files(dir, file_type='.txt'):\n",
    "    '''\n",
    "    获取文件名列表\n",
    "    :param dir: \n",
    "    :param file_type: \n",
    "    :return: \n",
    "    '''\n",
    "    file_list = []\n",
    "    for home, dirs, files in os.walk(dir):\n",
    "        for filename in files:\n",
    "            if file_type in filename:\n",
    "                file_list.append(os.path.join(home, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词法分析\n",
    "通过正则表达式对查询进行词法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 构造每种类型词的正则表达式，()代表分组，?P<NAME>为组命名\n",
    "token_or = r'(?P<OR>\\|\\|)'\n",
    "token_not = r'(?P<NOT>\\!)'\n",
    "token_word = r'(?P<WORD>[a-zA-Z]+)'\n",
    "token_and = r'(?P<AND>&&)'\n",
    "token_lp = r'(?P<LP>\\()'\n",
    "token_rp = r'(?P<RP>\\))'\n",
    "lexer = re.compile('|'.join([token_or, token_not, token_word,\n",
    "                            token_and, token_lp, token_rp]))  # 编译正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 用编译好的正则表达式进行词法分析\n",
    "def get_tokens(query):\n",
    "    \"\"\"\n",
    "    [('abc', 'WORD'), ('(', 'LP'), ('ab', 'WORD'), (')', 'RP'), ('hhh', 'WORD'), ('aa', 'WORD')]\n",
    "    :param query: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    tokens = []  # tokens中的元素类型为(token, token类型)\n",
    "    for token in re.finditer(lexer, query):\n",
    "        tokens.append((token.group(), token.lastgroup))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 布尔检索类\n",
    "由构建索引、布尔表达式解析、结果查询与合并三部分组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BoolRetrieval:\n",
    "    \"\"\"\n",
    "    布尔检索类\n",
    "    index为字典类型，其键为单词，值为文件ID列表，如{\"word\": [1, 2, 9], ...}\n",
    "    self.query_tokens为需要查询的词序列，list\n",
    "    self.files 文件名列表\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index_path=''):\n",
    "        '''\n",
    "\n",
    "        :param index_path:\n",
    "        '''\n",
    "        if index_path == '':\n",
    "            self.index = defaultdict(list)\n",
    "        # 已有构建好的索引文件\n",
    "        else:\n",
    "            data = np.load(index_path, allow_pickle=True)\n",
    "            self.files = data['files'][()]  # 没有很理解这后半部分【】的作用\n",
    "            self.index = data['index'][()]\n",
    "        self.query_tokens = []\n",
    "\n",
    "    def build_index(self, text_dir):\n",
    "        '''\n",
    "        self.index就像是一个列表，索引键为词，索引值为所在文档的序号\n",
    "        :param text_dir:\n",
    "        :return:\n",
    "        '''\n",
    "        self.files = get_files(text_dir)  # 获取所有文件名\n",
    "        for num in range(0, len(self.files)):\n",
    "            f = open(self.files[num])\n",
    "            text = f.read()\n",
    "            words = get_words(text)  # 分词\n",
    "            # 构建倒排索引\n",
    "            for word in words:\n",
    "                self.index[word].append(num)\n",
    "        print(self.files, self.index)\n",
    "        np.savez('index.npz', files=self.files, index=self.index)\n",
    "\n",
    "    def search(self, query):\n",
    "        self.query_tokens = get_tokens(query)  # 获取查询的tokens\n",
    "        result = []\n",
    "        # 将查询得到的文件ID转换成文件名\n",
    "        for num in self.evaluate(0, len(self.query_tokens) - 1):\n",
    "            result.append(self.files[num])\n",
    "        return result\n",
    "\n",
    "    # 递归解析布尔表达式，p、q为子表达式左右边界的下标\n",
    "    def evaluate(self, p, q):\n",
    "        # 解析错误\n",
    "        if p > q:\n",
    "            return []\n",
    "        # 单个token，一定为查询词\n",
    "        elif p == q:\n",
    "            return self.index[self.query_tokens[p][0]]\n",
    "        # 去掉外层括号\n",
    "        elif self.check_parentheses(p, q):\n",
    "            return self.evaluate(p + 1, q - 1)\n",
    "        # else:\n",
    "        #     # 非单个token\n",
    "        #     op,and_list = self.find_operator(p, q)\n",
    "        #     #只有一个and运算符\n",
    "        #     if op == -1:\n",
    "        #         return []\n",
    "        #     # files1为运算符左边得到的结果，files2为右边\n",
    "        #     if self.query_tokens[op][1] == 'NOT':\n",
    "        #         files1 = []\n",
    "        #     else:\n",
    "        #         files1 = self.evaluate(p, op - 1)\n",
    "        #     files2 = self.evaluate(op + 1, q)\n",
    "        #     return self.merge(files1, files2, self.query_tokens[op][1])\n",
    "\n",
    "        else:\n",
    "            # 非单个token\n",
    "            op, and_list = self.find_operator(p, q)\n",
    "            # 只有一个and运算符\n",
    "            if len(and_list) == 1 or not and_list:\n",
    "                if op == -1:\n",
    "                    return []\n",
    "                # files1为运算符左边得到的结果，files2为右边\n",
    "                if self.query_tokens[op][1] == 'NOT':\n",
    "                    files1 = []\n",
    "                else:\n",
    "                    files1 = self.evaluate(p, op - 1)\n",
    "                files2 = self.evaluate(op + 1, q)\n",
    "                return self.merge(files1, files2, self.query_tokens[op][1])\n",
    "            else:\n",
    "                if op == -1:\n",
    "                    return []\n",
    "                # files1为运算符左边得到的结果，files2为右边\n",
    "                if self.query_tokens[op][1] == 'NOT':\n",
    "                    files1 = []\n",
    "                elif self.query_tokens[op][1] == 'AND':  # 最小优先级为and，且有多个and\n",
    "\n",
    "                    # 将p和q加入列表，方便统一格式\n",
    "                    and_list.insert(0, p - 1)\n",
    "                    and_list.insert(len(and_list), q + 1)\n",
    "                    num_and = len(and_list)\n",
    "                    files = []\n",
    "                    # files.append(len(self.evaluate(p,and_list[0]-1)))\n",
    "                    i = 0\n",
    "                    while i < num_and - 1:\n",
    "                        files.append(len(self.evaluate(and_list[i] + 1, and_list[i + 1] - 1)))\n",
    "                        i += 1\n",
    "                    # files.append(len(self.evaluate(and_list[-1]+1,q)))\n",
    "                    print('各个段出现文章的个数分别为：',files)\n",
    "                    temp = pd.Series(files).sort_values()[:2].values\n",
    "                    print('即将进行运算的是以下两个段：',temp)\n",
    "                    files1 = self.evaluate(and_list[temp[0]] + 1, and_list[temp[0] + 1] - 1)\n",
    "                    files2 = self.evaluate(and_list[temp[1]] + 1, and_list[temp[1] + 1] - 1)\n",
    "                    return self.merge(files1, files2, self.query_tokens[op][1])\n",
    "\n",
    "\n",
    "                else:\n",
    "                    files1 = self.evaluate(p, op - 1)\n",
    "                files2 = self.evaluate(op + 1, q)\n",
    "                return self.merge(files1, files2, self.query_tokens[op][1])\n",
    "\n",
    "    # 判断表达式是否为 (expr)\n",
    "    # 判断表达式是否为 (expr)\n",
    "    def check_parentheses(self, p, q):\n",
    "        \"\"\"\n",
    "        判断表达式是否为 (expr)\n",
    "        整个表达式的左右括号必须匹配才为合法的表达式\n",
    "        返回True或False\n",
    "\n",
    "        #注意(based!day)&&(based&&day)\n",
    "        \"\"\"\n",
    "        LP = 0\n",
    "        if (self.query_tokens[p][0] == '(') & (self.query_tokens[q][0] == ')'):\n",
    "            i = p + 1\n",
    "            while (i < q):\n",
    "                if self.query_tokens[i][0] == ')':\n",
    "                    LP -= 1\n",
    "                    if LP < 0: return False\n",
    "                elif self.query_tokens[i][0] == '(':\n",
    "                    LP += 1\n",
    "                i += 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 寻找表达式的dominant的运算符（优先级最低）\n",
    "    def find_operator(self, p, q):\n",
    "        \"\"\"\n",
    "        寻找表达式的dominant的运算符（优先级最低）\n",
    "        其必定在括号外面（不存在整个子表达式被括号包围，前面以已处理）\n",
    "        返回dominant运算符的下标位置\n",
    "        \"\"\"\n",
    "        # 找出第一个非括号运算符，并将其位置记录在列表中\n",
    "        LP = 0\n",
    "        lowest_op = 'NOT'\n",
    "        first_and = -1\n",
    "        first_not = -1\n",
    "        and_list = []\n",
    "        while p < q:\n",
    "            temp_token = self.query_tokens[p][1]\n",
    "            if temp_token != 'WORD':\n",
    "                if temp_token == 'LP':\n",
    "                    LP += 1\n",
    "                elif temp_token == 'RP':\n",
    "                    LP -= 1\n",
    "                elif LP == 0:\n",
    "                    if temp_token == 'OR':\n",
    "                        return p, []\n",
    "                    elif temp_token == 'AND':\n",
    "                        lowest_op = 'AND'\n",
    "                        if first_and == -1: first_and = p\n",
    "                        and_list.append(p)\n",
    "                    elif temp_token == 'NOT':\n",
    "                        if first_not == -1: first_not = p\n",
    "\n",
    "            p += 1\n",
    "        if lowest_op == 'AND':\n",
    "            return first_and, and_list\n",
    "        else:\n",
    "            return first_not, []\n",
    "\n",
    "    def merge(self, files1, files2, op_type):\n",
    "        \"\"\"\n",
    "        根据运算符对进行相应的操作\n",
    "        在Python中可以通过集合的操作来实现\n",
    "        但为了练习算法，请遍历files1, files2合并\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        if op_type == 'AND':\n",
    "            # result = list(set(files1) & set(files2))\n",
    "            result = [item for item in files1 if item in files2]\n",
    "        elif op_type == \"OR\":\n",
    "            # result = list(set(files1) | set(files2))\n",
    "            temp_file_list = files1 + files2\n",
    "            if temp_file_list:\n",
    "                result = pd.array(temp_file_list).unique().to_numpy().tolist()\n",
    "        elif op_type == \"NOT\":\n",
    "            # result = list(set(range(0, len(self.files))) - set(files2))\n",
    "            # all_files=np.array()\n",
    "\n",
    "            result = [item for item in range(0, len(self.files)) if item not in files2]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建布尔检索类对象\n",
    "第一次需要调用build_index()函数创建索引，之后可直接用索引文件进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['text\\\\advantages.txt', 'text\\\\bir.txt', 'text\\\\disadvantage.txt',\n       'text\\\\ir.txt'], dtype='<U21')"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 221
    }
   ],
   "source": [
    "# br = BoolRetrieval()\n",
    "# br.build_index('text')\n",
    "br = BoolRetrieval('index.npz')\n",
    "br.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'formalism': [0],\n             'advantages': [0],\n             'intuitive': [0],\n             'clean': [0],\n             'easy': [0],\n             'concept': [0],\n             'implement': [0],\n             'terms': [1, 2],\n             'conceived': [1],\n             'many': [1, 2],\n             'information': [1, 2, 3],\n             'used': [1],\n             'boolean': [1, 2],\n             'logic': [1],\n             'contain': [1],\n             'theory': [1],\n             'sets': [1],\n             'retrieval': [1, 2, 3],\n             'time': [1],\n             'whether': [1],\n             'first': [1],\n             'searched': [1],\n             'ir': [1, 3],\n             'standard': [1],\n             'one': [1],\n             'bir': [1],\n             'set': [1],\n             'based': [1, 2, 3],\n             'day': [1],\n             'documents': [1, 2, 3],\n             'classical': [1],\n             'query': [1, 2],\n             'model': [1, 2],\n             'systems': [1],\n             'adopted': [1],\n             'user': [1, 2],\n             'simplistic': [2],\n             'queries': [2],\n             'translated': [2],\n             'expression': [2],\n             'formulated': [2],\n             'absence': [2],\n             'equally': [2],\n             'retrieve': [2],\n             'decision': [2],\n             'need': [2, 3],\n             'may': [2],\n             'frequently': [2],\n             'ranking': [2],\n             'partial': [2],\n             'exact': [2],\n             'hard': [2],\n             'binary': [2],\n             'provided': [2],\n             'awkward': [2],\n             'data': [2, 3],\n             'users': [2],\n             'returns': [2],\n             'like': [2],\n             'translate': [2],\n             'scale': [2],\n             'often': [2],\n             'notion': [2],\n             'grading': [2],\n             'find': [2],\n             'response': [2],\n             'weighted': [2],\n             'disadvantages': [2],\n             'either': [2],\n             'criteria': [2],\n             'matching': [2],\n             'indexing': [3],\n             'activity': [3],\n             'obtaining': [3],\n             'searching': [3],\n             'resources': [3],\n             'science': [3],\n             'full': [3],\n             'describes': [3],\n             'relevant': [3],\n             'also': [3],\n             'system': [3],\n             'metadata': [3],\n             'text': [3],\n             'texts': [3],\n             'databases': [3],\n             'content': [3],\n             'document': [3],\n             'searches': [3],\n             'collection': [3],\n             'images': [3],\n             'sounds': [3]})"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 222
    }
   ],
   "source": [
    "br.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "'boolean': [1, 2],\n",
    "\n",
    "'based': [1, 2, 3],\n",
    "\n",
    " 'day': [1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['text\\\\disadvantage.txt', 'text\\\\ir.txt']\n",
      "['text\\\\bir.txt']\n",
      "['text\\\\bir.txt', 'text\\\\disadvantage.txt']\n",
      "[]\n",
      "['text\\\\disadvantage.txt', 'text\\\\ir.txt', 'text\\\\bir.txt']\n",
      "['text\\\\disadvantage.txt', 'text\\\\ir.txt', 'text\\\\bir.txt']\n",
      "\n",
      "以下测试多个and的优化：\n",
      "各个段出现文章的个数分别为： [3, 1, 2]\n",
      "即将进行运算的是以下两个段： [1 2]\n",
      "['text\\\\bir.txt']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 'boolean': [1, 2],\n",
    "# \n",
    "# 'based': [1, 2, 3],\n",
    "# \n",
    "#  'day': [1],\n",
    "# query = input(\"请输入与查询（或||，与&&，非！）：\")\n",
    "print(br.search('based&&!day'))\n",
    "print(br.search('based&&day'))\n",
    "print(br.search('day||boolean'))\n",
    "print(br.search('(based&&!day)&&(based&&day)'))\n",
    "print(br.search('(based&&!day)||(based&&day)'))\n",
    "print(br.search('((based&&!day)||boolean)||(based&&day)'))\n",
    "print('\\n以下测试多个and的优化：')\n",
    "print(br.search('based&&day&&boolean'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}